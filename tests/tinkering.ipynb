{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e274943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tinkering.ipynb or tinkering.py\n",
    "\"\"\"\n",
    "Initialize and test LLMService in notebook environment\n",
    "\"\"\"\n",
    "\n",
    "# Fix import paths for your current structure\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.dirname(os.path.abspath('.'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Option 1: Simple settings without pydantic\n",
    "class SimpleSettings:\n",
    "    def __init__(self):\n",
    "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "        self.groq_model = \"llama3-8b-8192\"\n",
    "        self.groq_temperature = 0.1\n",
    "        self.groq_max_tokens = 1000\n",
    "\n",
    "# Create settings instance\n",
    "settings = SimpleSettings()\n",
    "\n",
    "# Now import and initialize LLMService\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from groq import Groq\n",
    "\n",
    "# Set up logging for notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LLMService:\n",
    "    \"\"\"Service for LLM operations using Groq\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initialize LLM service with Groq client\"\"\"\n",
    "        self.api_key = api_key or settings.groq_api_key\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable.\")\n",
    "        \n",
    "        self.client = Groq(api_key=self.api_key)\n",
    "        self.model = settings.groq_model\n",
    "        self.temperature = settings.groq_temperature\n",
    "        self.max_tokens = settings.groq_max_tokens\n",
    "        \n",
    "        logger.info(f\"LLM service initialized with model: {self.model}\")\n",
    "    \n",
    "    def generate_answer(self, question: str, context_chunks: List[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"Generate answer using Groq LLM with optional context\"\"\"\n",
    "        try:\n",
    "            if context_chunks:\n",
    "                context = self._format_context(context_chunks)\n",
    "                prompt = self._create_prompt(question, context)\n",
    "            else:\n",
    "                prompt = question\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful AI assistant. Answer questions clearly and concisely.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens,\n",
    "                top_p=1,\n",
    "                stream=False\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            logger.info(f\"Generated answer for question: {question[:50]}...\")\n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {str(e)}\")\n",
    "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def _format_context(self, context_chunks: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Format context chunks into a readable string\"\"\"\n",
    "        if not context_chunks:\n",
    "            return \"No relevant context found.\"\n",
    "        \n",
    "        formatted_context = []\n",
    "        for i, chunk in enumerate(context_chunks, 1):\n",
    "            source = chunk.get('metadata', {}).get('source', 'Unknown')\n",
    "            text = chunk.get('text', '')\n",
    "            formatted_context.append(f\"[Context {i} - Source: {source}]\\n{text}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_context)\n",
    "    \n",
    "    def _create_prompt(self, question: str, context: str) -> str:\n",
    "        \"\"\"Create a well-structured prompt for the LLM\"\"\"\n",
    "        return f\"\"\"Based on the following context, please answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    def simple_chat(self, message: str) -> str:\n",
    "        \"\"\"Simple chat without context - useful for testing\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": message\n",
    "                    }\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "# ============================================================================\n",
    "# NOTEBOOK TESTING SECTION\n",
    "# ============================================================================\n",
    "\n",
    "def test_llm_service():\n",
    "    \"\"\"Test the LLM service initialization and basic functionality\"\"\"\n",
    "    \n",
    "    print(\"=== Testing LLM Service ===\\n\")\n",
    "    \n",
    "    # Check if API key is set\n",
    "    if not os.getenv(\"GROQ_API_KEY\"):\n",
    "        print(\"‚ùå GROQ_API_KEY not found in environment variables\")\n",
    "        print(\"Please set it with: os.environ['GROQ_API_KEY'] = 'your-api-key'\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM service\n",
    "        print(\"üöÄ Initializing LLM service...\")\n",
    "        llm_service = LLMService()\n",
    "        print(\"‚úÖ LLM service initialized successfully!\")\n",
    "        \n",
    "        # Test simple chat\n",
    "        print(\"\\nüìù Testing simple chat...\")\n",
    "        test_message = \"Hello! Can you tell me what 2+2 equals?\"\n",
    "        response = llm_service.simple_chat(test_message)\n",
    "        print(f\"Question: {test_message}\")\n",
    "        print(f\"Answer: {response}\")\n",
    "        \n",
    "        # Test with context\n",
    "        print(\"\\nüìö Testing with context...\")\n",
    "        sample_context = [\n",
    "            {\n",
    "                \"text\": \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
    "                \"metadata\": {\"source\": \"ai_textbook.pdf\"}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        context_question = \"What is machine learning?\"\n",
    "        context_answer = llm_service.generate_answer(context_question, sample_context)\n",
    "        print(f\"Question: {context_question}\")\n",
    "        print(f\"Answer: {context_answer}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ All tests passed!\")\n",
    "        return llm_service\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error initializing LLM service: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TESTS\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\" or \"ipykernel\" in sys.modules:\n",
    "    # Set your API key here if not in environment\n",
    "    # os.environ[\"GROQ_API_KEY\"] = \"your-groq-api-key-here\"\n",
    "    \n",
    "    # Run the test\n",
    "    llm = test_llm_service()\n",
    "    \n",
    "    # If successful, you can now use llm for further testing\n",
    "    if llm:\n",
    "        print(\"\\nüéâ LLM service ready for use!\")\n",
    "        print(\"You can now call:\")\n",
    "        print(\"- llm.simple_chat('your question')\")\n",
    "        print(\"- llm.generate_answer('question', context_chunks)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50120e40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
