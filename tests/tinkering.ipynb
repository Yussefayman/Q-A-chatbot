{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e274943",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:LLM service initialized with model: llama3-8b-8192\n",
      "INFO:httpx:HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Generated answer for question: What is the capital of France?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# tinkering.ipynb or tinkering.py\n",
    "\"\"\"\n",
    "Initialize and test LLMService in notebook environment\n",
    "\"\"\"\n",
    "\n",
    "# Fix import paths for your current structure\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to Python path\n",
    "project_root = os.path.dirname(os.path.abspath('.'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Option 1: Simple settings without pydantic\n",
    "class SimpleSettings:\n",
    "    def __init__(self):\n",
    "        self.groq_api_key = os.getenv(\"GROQ_API_KEY\", \"\")\n",
    "        self.groq_model = \"llama3-8b-8192\"\n",
    "        self.groq_temperature = 0.1\n",
    "        self.groq_max_tokens = 1000\n",
    "\n",
    "# Create settings instance\n",
    "settings = SimpleSettings()\n",
    "\n",
    "# Now import and initialize LLMService\n",
    "import logging\n",
    "from typing import List, Dict, Any\n",
    "from groq import Groq\n",
    "\n",
    "# Set up logging for notebook\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class LLMService:\n",
    "    \"\"\"Service for LLM operations using Groq\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key: str = None):\n",
    "        \"\"\"Initialize LLM service with Groq client\"\"\"\n",
    "        self.api_key = api_key or settings.groq_api_key\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"Groq API key is required. Set GROQ_API_KEY environment variable.\")\n",
    "        \n",
    "        self.client = Groq(api_key=self.api_key)\n",
    "        self.model = settings.groq_model\n",
    "        self.temperature = settings.groq_temperature\n",
    "        self.max_tokens = settings.groq_max_tokens\n",
    "        \n",
    "        logger.info(f\"LLM service initialized with model: {self.model}\")\n",
    "    \n",
    "    def generate_answer(self, question: str, context_chunks: List[Dict[str, Any]] = None) -> str:\n",
    "        \"\"\"Generate answer using Groq LLM with optional context\"\"\"\n",
    "        try:\n",
    "            if context_chunks:\n",
    "                context = self._format_context(context_chunks)\n",
    "                prompt = self._create_prompt(question, context)\n",
    "            else:\n",
    "                prompt = question\n",
    "            \n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are a helpful AI assistant. Answer questions clearly and concisely.\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens,\n",
    "                top_p=1,\n",
    "                stream=False\n",
    "            )\n",
    "            \n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            logger.info(f\"Generated answer for question: {question[:50]}...\")\n",
    "            return answer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating answer: {str(e)}\")\n",
    "            return f\"I apologize, but I encountered an error: {str(e)}\"\n",
    "    \n",
    "    def _format_context(self, context_chunks: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"Format context chunks into a readable string\"\"\"\n",
    "        if not context_chunks:\n",
    "            return \"No relevant context found.\"\n",
    "        \n",
    "        formatted_context = []\n",
    "        for i, chunk in enumerate(context_chunks, 1):\n",
    "            source = chunk.get('metadata', {}).get('source', 'Unknown')\n",
    "            text = chunk.get('text', '')\n",
    "            formatted_context.append(f\"[Context {i} - Source: {source}]\\n{text}\")\n",
    "        \n",
    "        return \"\\n\\n\".join(formatted_context)\n",
    "    \n",
    "    def _create_prompt(self, question: str, context: str) -> str:\n",
    "        \"\"\"Create a well-structured prompt for the LLM\"\"\"\n",
    "        return f\"\"\"Based on the following context, please answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    def simple_chat(self, message: str) -> str:\n",
    "        \"\"\"Simple chat without context - useful for testing\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": message\n",
    "                    }\n",
    "                ],\n",
    "                temperature=self.temperature,\n",
    "                max_tokens=self.max_tokens\n",
    "            )\n",
    "            \n",
    "            return response.choices[0].message.content.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "\n",
    "# ============================================================================\n",
    "# NOTEBOOK TESTING SECTION\n",
    "# ============================================================================\n",
    "\n",
    "def test_llm_service():\n",
    "    \"\"\"Test the LLM service initialization and basic functionality\"\"\"\n",
    "    \n",
    "    print(\"=== Testing LLM Service ===\\n\")\n",
    "    \n",
    "    # Check if API key is set\n",
    "    if not os.getenv(\"GROQ_API_KEY\"):\n",
    "        print(\"âŒ GROQ_API_KEY not found in environment variables\")\n",
    "        print(\"Please set it with: os.environ['GROQ_API_KEY'] = 'your-api-key'\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM service\n",
    "        print(\"ðŸš€ Initializing LLM service...\")\n",
    "        llm_service = LLMService()\n",
    "        print(\"âœ… LLM service initialized successfully!\")\n",
    "        \n",
    "        # Test simple chat\n",
    "        print(\"\\nðŸ“ Testing simple chat...\")\n",
    "        test_message = \"Hello! Can you tell me what 2+2 equals?\"\n",
    "        response = llm_service.simple_chat(test_message)\n",
    "        print(f\"Question: {test_message}\")\n",
    "        print(f\"Answer: {response}\")\n",
    "        \n",
    "        # Test with context\n",
    "        print(\"\\nðŸ“š Testing with context...\")\n",
    "        sample_context = [\n",
    "            {\n",
    "                \"text\": \"Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\",\n",
    "                \"metadata\": {\"source\": \"ai_textbook.pdf\"}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        context_question = \"What is machine learning?\"\n",
    "        context_answer = llm_service.generate_answer(context_question, sample_context)\n",
    "        print(f\"Question: {context_question}\")\n",
    "        print(f\"Answer: {context_answer}\")\n",
    "        \n",
    "        print(\"\\nâœ… All tests passed!\")\n",
    "        return llm_service\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error initializing LLM service: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# ============================================================================\n",
    "# RUN TESTS\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\" or \"ipykernel\" in sys.modules:\n",
    "    llm = LLMService()\n",
    "    print(llm.generate_answer(\"What is the capital of France?\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50120e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n",
      "ERROR:chromadb.telemetry.product.posthog:Failed to send telemetry event ClientCreateCollectionEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in an existing event loop (like Jupyter)\n",
      "Please run the server manually with: uvicorn main:app --reload --host 0.0.0.0 --port 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yusse\\AppData\\Local\\Temp\\ipykernel_27860\\4174945015.py:131: DeprecationWarning: \n",
      "        on_event is deprecated, use lifespan event handlers instead.\n",
      "\n",
      "        Read more about it in the\n",
      "        [FastAPI docs for Lifespan Events](https://fastapi.tiangolo.com/advanced/events/).\n",
      "        \n",
      "  @app.on_event(\"startup\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import jwt\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Optional, List\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "from fastapi import FastAPI, HTTPException, Depends, UploadFile, File, Form\n",
    "from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials\n",
    "from pydantic import BaseModel\n",
    "import chromadb\n",
    "from groq import Groq\n",
    "import docling\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "# Configuration\n",
    "SECRET_KEY = \"your-secret-key-change-this\"\n",
    "GROQ_API_KEY = \"your-groq-api-key\"\n",
    "ALGORITHM = \"HS256\"\n",
    "ACCESS_TOKEN_EXPIRE_MINUTES = 30\n",
    "\n",
    "# Initialize\n",
    "app = FastAPI(title=\"QA API\", version=\"1.0.0\")\n",
    "security = HTTPBearer()\n",
    "groq_client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# Initialize ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "collection = chroma_client.get_or_create_collection(name=\"documents\")\n",
    "\n",
    "# Document converter\n",
    "converter = DocumentConverter()\n",
    "\n",
    "# Models\n",
    "class UserCreate(BaseModel):\n",
    "    email: str\n",
    "    password: str\n",
    "\n",
    "class UserLogin(BaseModel):\n",
    "    email: str\n",
    "    password: str\n",
    "\n",
    "class Question(BaseModel):\n",
    "    question: str\n",
    "\n",
    "class Token(BaseModel):\n",
    "    access_token: str\n",
    "    token_type: str\n",
    "\n",
    "# Database setup\n",
    "def init_db():\n",
    "    conn = sqlite3.connect('qa_app.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Users table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS users (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            email TEXT UNIQUE NOT NULL,\n",
    "            password_hash TEXT NOT NULL,\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Logs table\n",
    "    cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS query_logs (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            user_id INTEGER,\n",
    "            question TEXT NOT NULL,\n",
    "            response TEXT NOT NULL,\n",
    "            response_time REAL NOT NULL,\n",
    "            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            FOREIGN KEY (user_id) REFERENCES users (id)\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Utility functions\n",
    "def hash_password(password: str) -> str:\n",
    "    return hashlib.sha256(password.encode()).hexdigest()\n",
    "\n",
    "def verify_password(password: str, hashed: str) -> bool:\n",
    "    return hash_password(password) == hashed\n",
    "\n",
    "def create_access_token(data: dict):\n",
    "    to_encode = data.copy()\n",
    "    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)\n",
    "    to_encode.update({\"exp\": expire})\n",
    "    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)\n",
    "    return encoded_jwt\n",
    "\n",
    "def get_user_from_token(credentials: HTTPAuthorizationCredentials = Depends(security)):\n",
    "    try:\n",
    "        payload = jwt.decode(credentials.credentials, SECRET_KEY, algorithms=[ALGORITHM])\n",
    "        email: str = payload.get(\"sub\")\n",
    "        if email is None:\n",
    "            raise HTTPException(status_code=401, detail=\"Invalid token\")\n",
    "        return email\n",
    "    except jwt.ExpiredSignatureError:\n",
    "        raise HTTPException(status_code=401, detail=\"Token expired\")\n",
    "    except jwt.JWTError:\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid token\")\n",
    "\n",
    "def get_user_id(email: str) -> int:\n",
    "    conn = sqlite3.connect('qa_app.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT id FROM users WHERE email = ?\", (email,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    return result[0] if result else None\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
    "    \"\"\"Simple text chunking with overlap\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        if end >= len(text):\n",
    "            break\n",
    "        start = end - overlap\n",
    "    return chunks\n",
    "\n",
    "# API Endpoints\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    init_db()\n",
    "\n",
    "@app.post(\"/register\", response_model=Token)\n",
    "async def register(user: UserCreate):\n",
    "    conn = sqlite3.connect('qa_app.db')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Check if user exists\n",
    "    cursor.execute(\"SELECT id FROM users WHERE email = ?\", (user.email,))\n",
    "    if cursor.fetchone():\n",
    "        conn.close()\n",
    "        raise HTTPException(status_code=400, detail=\"Email already registered\")\n",
    "    \n",
    "    # Create user\n",
    "    password_hash = hash_password(user.password)\n",
    "    cursor.execute(\n",
    "        \"INSERT INTO users (email, password_hash) VALUES (?, ?)\",\n",
    "        (user.email, password_hash)\n",
    "    )\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    \n",
    "    # Create token\n",
    "    access_token = create_access_token(data={\"sub\": user.email})\n",
    "    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n",
    "\n",
    "@app.post(\"/login\", response_model=Token)\n",
    "async def login(user: UserLogin):\n",
    "    conn = sqlite3.connect('qa_app.db')\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT password_hash FROM users WHERE email = ?\", (user.email,))\n",
    "    result = cursor.fetchone()\n",
    "    conn.close()\n",
    "    \n",
    "    if not result or not verify_password(user.password, result[0]):\n",
    "        raise HTTPException(status_code=401, detail=\"Invalid credentials\")\n",
    "    \n",
    "    access_token = create_access_token(data={\"sub\": user.email})\n",
    "    return {\"access_token\": access_token, \"token_type\": \"bearer\"}\n",
    "\n",
    "@app.post(\"/upload\")\n",
    "async def upload_document(\n",
    "    file: UploadFile = File(...),\n",
    "    current_user: str = Depends(get_user_from_token)\n",
    "):\n",
    "    # Check file size (10MB limit)\n",
    "    if file.size > 10 * 1024 * 1024:\n",
    "        raise HTTPException(status_code=400, detail=\"File too large. Max size: 10MB\")\n",
    "    \n",
    "    # Check file type\n",
    "    if not file.filename.endswith(('.txt', '.pdf')):\n",
    "        raise HTTPException(status_code=400, detail=\"Only .txt and .pdf files are supported\")\n",
    "    \n",
    "    try:\n",
    "        # Save uploaded file temporarily\n",
    "        temp_path = f\"temp_{uuid.uuid4()}_{file.filename}\"\n",
    "        with open(temp_path, \"wb\") as f:\n",
    "            content = await file.read()\n",
    "            f.write(content)\n",
    "        \n",
    "        # Convert document using docling\n",
    "        result = converter.convert(temp_path)\n",
    "        text_content = result.document.export_to_markdown()\n",
    "        \n",
    "        # Clean up temp file\n",
    "        os.remove(temp_path)\n",
    "        \n",
    "        # Chunk the text\n",
    "        chunks = chunk_text(text_content)\n",
    "        \n",
    "        # Store in ChromaDB\n",
    "        chunk_ids = [f\"{file.filename}_{i}\" for i in range(len(chunks))]\n",
    "        metadatas = [{\"filename\": file.filename, \"chunk_index\": i} for i in range(len(chunks))]\n",
    "        \n",
    "        collection.add(\n",
    "            documents=chunks,\n",
    "            ids=chunk_ids,\n",
    "            metadatas=metadatas\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"message\": f\"Document '{file.filename}' uploaded successfully\",\n",
    "            \"chunks_created\": len(chunks)\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Clean up temp file if it exists\n",
    "        if os.path.exists(temp_path):\n",
    "            os.remove(temp_path)\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing document: {str(e)}\")\n",
    "\n",
    "@app.post(\"/ask\")\n",
    "async def ask_question(\n",
    "    question: Question,\n",
    "    current_user: str = Depends(get_user_from_token)\n",
    "):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Search for relevant chunks\n",
    "        results = collection.query(\n",
    "            query_texts=[question.question],\n",
    "            n_results=3\n",
    "        )\n",
    "        \n",
    "        if not results['documents'][0]:\n",
    "            raise HTTPException(status_code=404, detail=\"No relevant documents found\")\n",
    "        \n",
    "        # Prepare context from retrieved chunks\n",
    "        context = \"\\n\\n\".join(results['documents'][0])\n",
    "        \n",
    "        # Create prompt for Groq\n",
    "        prompt = f\"\"\"Based on the following context, answer the user's question. If the answer cannot be found in the context, say so clearly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question.question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        # Get response from Groq\n",
    "        response = groq_client.chat.completions.create(\n",
    "            model=\"llama3-8b-8192\",  # or another available model\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500,\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        answer = response.choices[0].message.content\n",
    "        response_time = time.time() - start_time\n",
    "        \n",
    "        # Log the query\n",
    "        user_id = get_user_id(current_user)\n",
    "        conn = sqlite3.connect('qa_app.db')\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\n",
    "            \"INSERT INTO query_logs (user_id, question, response, response_time) VALUES (?, ?, ?, ?)\",\n",
    "            (user_id, question.question, answer, response_time)\n",
    "        )\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        return {\n",
    "            \"question\": question.question,\n",
    "            \"answer\": answer,\n",
    "            \"response_time\": response_time,\n",
    "            \"sources\": len(results['documents'][0])\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=f\"Error processing question: {str(e)}\")\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health_check():\n",
    "    return {\"status\": \"healthy\"}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    import asyncio\n",
    "    import sys\n",
    "    \n",
    "    # Check if we're in a Jupyter environment or already have an event loop\n",
    "    try:\n",
    "        # Try to get the current event loop\n",
    "        loop = asyncio.get_running_loop()\n",
    "        print(\"Running in an existing event loop (like Jupyter)\")\n",
    "        print(\"Please run the server manually with: uvicorn main:app --reload --host 0.0.0.0 --port 8000\")\n",
    "    except RuntimeError:\n",
    "        # No event loop running, safe to use uvicorn.run()\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "078e3e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Will watch for changes in these directories: ['c:\\\\Q&A\\\\tests']\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Started reloader process [27860] using WatchFiles\n",
      "INFO:     Stopping reloader process [27860]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Simple server runner to avoid asyncio conflicts\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    \n",
    "    # Run the server\n",
    "    uvicorn.run(\n",
    "        \"main:app\",\n",
    "        host=\"0.0.0.0\",\n",
    "        port=8000,\n",
    "        reload=True,\n",
    "        log_level=\"info\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ca9aa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
